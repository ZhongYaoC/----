##《分布式系统与一致性》

无状态服务器：应用服务器中不再保存任何数据；数据全部保存在专门的数据库服务器



### 系统架构变迁

单机

垂直

水平

分布式特性：一致性、可扩展、容错、高可用等

其中一致性最为重要的，一致性的变动会连带影响其他特性





### Paxos算法（共识算法）

proposer：提出value

acceptor：会**保存**确定下来的value

paxos在于多个acceptor之间达成某种共识，而不是在于proposer，而且也只有acceptor才会存储这个共识



#### Basic Paxos（paxos consensus algorithm）

**一次paxos consensus只能批准一个value！！！- - 达成共识（这也是最终目的所在！）**，但每次paxos consensus有多轮，只要最终的学习值阶段没有达成所有的一致，就继续一轮一轮的做下去。

#####选择值（2阶段4步）：

**值（value）代表的是某种操作，而不是简单的一个字面意义上的变量**，而且可以是任意操作，例如“修改某个变量的值为某个值”、“设置当前 primary 为某个节点”等。Paxos协议中统一将这些操作抽象为 value。

选择值过程是proposer和大多数（majority）的acceptor之间的交互 

1a. proposer向majority acceptor 提出prepare request，并声明当前提议号n

1b. acceptor收到prepare后，检查是否response过更高的prepare，否则回复proposer：承诺不会接受提议号小于n的提议；同时返回已接受的最大的提议（含提议号和提议值）-如果有的话，没有就返回null；promise(n,an,av)

> Proposer 应该是收到半数以上promise拒绝后才可以退出重新开始prepare，而不是只要有promise拒绝就退出重启prepare。例子：
>
> 

2a. proposer收到半数以上acceptor的promise后，发出accept request(n, v)，但其中的值有两种：

1）收到的promise全部都是null，即acceptor都没有接受过其他提议，则可自由任选值

2）收到的promise中有已接受的值，则后者认可前者，将值替换为接受的小于n的提议中最大的提议值

**PS：单纯只替换提议值是不够的，还需要更新该提议值被accepted时的提议号**

>如频繁宕机情况：
>
>

2b. 收到提议号为n的accept  request后，如果没有回复过其他更高的提议号就接受该值（存储），否则拒绝该请求

（有些类似于两阶段提交的概念？？）

最后一步的acceptor收到accept request后存储新值（表明被批准accepted了），不需要验证是大多数acceptor收到了，可能只有一个acceptor收到了；不过在之后的学习值过程中会要求大多数的acceptor给learner发送了值，从而反向验证了收到accept request并存储下来的acceptor是majority



###### Tips

1、选择值的过程中，可能导致“不一致”，但再经过几轮的提议后，最终达成一致，确定某个value；

2、前一个proposer的prepare request可能会被另一个proposer的prepare request直接取消，从而提出了一个更高提议号的prepare request，但又取消了另一个刚提出的prepare，这样不断的相互取消就产生了活锁（对于活锁，我的理解是它不像死锁，陷入相互等待资源，必须等新资源补充；活锁是只要其中一个proposer稍缓或者停下来，就可以恢复正常，因为活锁产生的原因在于：高轮数的提议可以抢占低轮数的提议，而这又恰恰是为了防止死锁的产生）

所以为避免活锁，可规定只有一个proposer可以提议，这个proposer即为distinguished proposer（但这样不是变成类似于只有一个提议者，然后提议之后再广播给所有的模式了吗？还需要共识吗？老师：共识的关键在于又多个acceptor，而不是又多个proposer）

3、选择值过程中，**后者认同前者**，防止出现后者提出的新值覆盖原有被批准的旧值，始终无法达成一致

##### 学习值

选择值之后，只有大多数（majority）的acceptor接受了值，学习值过程，即将选中的值告知所有的learner；

默认是这大多数的acceptor会给所有的learner发送值，所以消息总数为nums of majority acceptor * num of learner，网络占用较大，所以可以选择某一个learner为distinguished learner，这大多数的acceptor将值发给distinguished  learner，该learner收到大多数acceptor的值后会向剩余的所有learner发送该值，消息总数为nums of majority  acceptor + nums of learner；

实践中，选择某进程，该进程的proposer成为distinguished proposer，learner成为distinguished learner，这个被选中的进程即为**leader**



**paxos learn的意义** 类似zab中的commit //edit at 2022-03-10

paxos完成2b阶段后，参与paxos的半数以上acceptor只是存储了这个value，并不明确知道这个value被确定下来，成为了最终值；所以需要广播出去，learner收到半数以上的相同value后，说明该值即为最终值，否则value仍未选定；改进版的learn过程将判断半数以上的工作提前了，交给leader去判断，判断完成就直接广播，learner只需要学习。

zab中commit与此含义类似。raft中的commit代表将日志提交至状态机执行





#### Multi Paxos（paxos algorithm / state machine）

等同于paxos consensus的多次执行，每次paxos consensus的执行即为一个实例，每个实例拥有一个实例编号

1、独立实例的paxos

2、共用第一次prepare的paxos



#### 复制状态机 replicated deterministic state machine





目前问题：paxos的应用 -- 选择值和学习值具体怎么用到分布式场景，多个acceptor达成共识然后呢？复制状态机让多个副本之间的命令顺序一致，paxos具体怎么做到？

Q2：

![PaxosQuestion1](F:\Markdown\研一上\图片\PaxosQuestion1.png)



kspace paxos代码先忽略存储、网络，关注paxos中存储了哪些部分，更新了哪些部分





#### Raft（复制日志算法）

与paxos比较的话，raft的term（任期）更像是一个跨多个实例、多次提议的选举分界线（或者leader分界线）

##### 投票策略（& election restriction）：

candidate先将自身term+1，然后发出RequestVote RPC，此RPC中会携带```term、日志中最后一条entry的term、日志中最后一条entry的index```；

follower首先判断自身currentTerm是否小于RPC中term，如果currentTerm > term，return false；

通过currentTerm判断后，检查RPC中最后一条entry的term和index与自身保存的最后一条entry的term及index的大小，RPC中term更大，则通过，两者term相同，则检查index，RPC中index更大，亦通过

也就是说 所谓“更新”的日志的评判标准是：该日志的term和index同时决定



##### 一致性检查

raft中认为日志具有特性：两个不同日志中的两个entry拥有相同的term和index，则这两个entry 前面的所有entry也全部相同；

为保证该特性，append entry时需要做一致性检查：raft中通过找到leader和follower两者最后达成一致的entry，然后删除follower在此entry之后的所有entry，用leader中的后续entry覆盖

###### 具体操作：

leader发送append RPC时会附带上当前entry的前一条entry的term和index，如果follower在自身entry中找不到RPC中附带的entry term和index，则返回append调用失败；leader继续把前一个entry发送给follower查找，直到append调用成功；如果append调用成功，则说明leader和follower已经找到最后达成一致的地点，leader从这个entry开始逐个调用append RPC，即用leader中的entry覆盖follower中原有的entry

故raft中不存在follower到leader的数据流动，leader不需要通过follower来保持数据的最新性



而为了标识leader为follower附加entry的位置，leader为每个follower维护一个nextIndex（直接一致性 检查时匹配的index+1，不行吗？？），当leader刚刚选出时，该值默认为leader的log最后一条entry的下一个位置，如果一致性检查未通过（即AppendEntriesRPC 返回false），则将nextIndex向前移动一位（修改相应前一条entry），再次发送AppendRPC，继续一致性检查，直到调用成功，将nextIndex对应entry覆盖至follower



##### not commiting entries from previous terms

新leader会保存旧leader时的entry，但不会提交这些已达多数的entry，当有新的entry到达时，并达到提交条件时，会自动提交起那么那些未提交的entry（根据一致性检查）；而提出这一标准的原因是：如果不这样做，会有已经被提交的日志被删除的情况出现；而此要求下，那些可能被删除的日志不会被过早的提交，也就是说raft满足了它自己提出的Leader Completeness：已被提交的日志不会被删除，但对那些已在大多数服务器达成一致但未提交的日志，认为是可以放弃的。

>新leader启动后如何判定后，旧的entry是否已提交？
>
>根据状态机是否是持久性的，如果状态机是持久性的，那么为防止重复的应用日志条目，服务器的lastApplied状态会被持久化，所以重启后，可以找到对应已经应用到状态机的索引（即说明新leader中也保存了该值）；而如果状态机是不持久的，那么服务器重启后会通过snapshot恢复，而snapshot中会持久化lastApplied，所以重启的服务器仍然拥有了最后应用的entry索引



##### 相关RPC

Figure 2的图看懂，搞清楚具体选举、复制策略；以及相应存储的数据

###### 存储状态

###### RequestVote RPC

###### AppendEntries RPC

同时用于追加entry和心跳包使用，心跳包时也需要做一致性检查







Figure8：

一个已经在多数节点中存储（还未commit）的entry仍然可能在之后被覆盖



#### ZAB（原子广播算法）

首要进程：

leader：



zab中没有提议号，epoch和zid更像是任期和实例号





zab需要保证在某个副本上已被投递的提议，在其他副本上也会被提交；

1、为什么会出现这个问题？

leader中执行提交操作和follower中提交操作是异步的。

zab 的leader在收到多数的ack后，即认为该提议为已提交状态，会执行对应提交操作，同时发送commit请求；而follower是在收到leader的commit请求后，知道状态为已提交状态，会执行提交操作。所以如果leader再执行完提交操作后宕机，commit请求未来及发出，则此提议在leader所在副本已经被投递，但其他副本处于未被投递（还没执行提交操作）。所以根源来自于leader和follower的提交异步

2、raft中如何处理这个问题？

raft中leader提交entry（即应用此entry至状态机）后宕机，而之前响应leader的follower还未提交；在新的一轮中选举中，若之前从未响应过旧leader的follower首先发起投票请求，根据raft的投票策略（election restriction），此投票请求会被拒绝，从而新的leader只会在响应过旧leader的majority follower中选出。然后根据raft不直接提交旧leader的entry的要求，此entry会在下一次entry提交时同时被提交。

论文Figure8例子，raft似乎是通过拒绝新leader提交旧term的entry，当有新的entry追加进入，达到提交条件后，间接提交旧entry的方式，**也就是说允许已达到多数的entry（但未提交）的entry被删除！！**；（~~但是无法规避，如只有leader提交了该entry，而后期新的leader直接覆盖这个已提交的entry，更加错误了！！~~，后期的leader只会在已经有entry的那几个节点产生）。在paxos中达到多数的提议值是不会被删除的，因为如果新的proposer提出新值，acceptor返回的promise(n, (旧值的提议号，旧值))；accept消息中会更新（必须更新，否则可能出现已占多数的提议值被删除）这个旧值对应的提议号，即accept(n，(n，旧值))；有些类似于在raft中已占多数但未提交的旧entry在新的term中，该entry的term会被更新；当然实际的raft放弃了这种方案，选择了保持entry的term始终不变。



上述可能存在问题，根据raft中描述的服务器所有拥有的状态信息 -- commitIndex和lastApplied，说明存在commit和apply两个概念，commit似乎更多的代表entry已占到大多数，可以提交至状态机；而apply是将已达到commit状态的entry真实提交至状态机运行。

感觉raft中一直在规避一个情况，一个在更大term中才达到大多数的entry，由于新leader不会commit旧的entry，而这时leader再次宕机，新leader选出后，是有可能覆盖这部分已占多数的entry，但这又不违背作者最初提出的Leader Completeness ` if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms.`因为这个达到多数的entry在原先的leader中根本没有被commit，但是按照paxos的概念，已经在acceptor中占到多数，那么就不能被覆盖掉！！





raft中存在的可能隐患：

假定现有三台服务器A、B、C，分别对应term和log为 (3,3) (2,2) (3,3)，log均已被提交，因为(3,3)已达到半数以上；但B节点未和A、C节点保持同步（可能由于leader和B节点之间存在暂时的网络故障或网络拥堵），如果此时三节点同时掉电宕机，并且原有的leader节点C磁盘故障，更换了磁盘后，原有持久化数据全部清空，即状态变为(0,0)，如果此时B节点首先发起选主，那么C节点会投赞同票，B节点将变为leader，等B节点将所有数据刷新至AC节点，将导致A节点的(3,3)数据被删除；从结果来看，(3,3)log属于已被提交的日志，但是仍然被删除，这与raft论文中的Leader Completeness 相悖，而究其原因：数据被清空的C节点参与了投票，而C节点由于数据被清空，它对于收到的所有RequestVote均会赞同，这就导致新的leader不一定是数据最新的。

为规避此类现象，应该对状态为0的节点做出投票限制，如果所有节点状态均为0，那么正常发起投票，因为这种情况是raft集群的最初启动场景；而如果只有少数节点状态为0，那么此0节点应该被限制参与；如果半数以上的节点状态为0，似乎也可以正常参与选举投票。

为实现这种策略，每个节点应该具备全局的状态信息，即log[]

（这让我想到redis的选举策略？？）