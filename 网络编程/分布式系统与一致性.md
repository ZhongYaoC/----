##《分布式系统与一致性》

无状态服务器：应用服务器中不再保存任何数据；数据全部保存在专门的数据库服务器



### 系统架构变迁

单机

垂直

水平

分布式特性：一致性、可扩展、容错、高可用等

其中一致性最为重要的，一致性的变动会连带影响其他特性





### Paxos算法（共识算法）

proposer：提出value

acceptor：会**保存**确定下来的value

paxos在于多个acceptor之间达成某种共识，而不是在于proposer，而且也只有acceptor才会存储这个共识



#### Basic Paxos（paxos consensus algorithm）

**一次paxos consensus只能批准一个value！！！- - 达成共识（这也是最终目的所在！）**，但每次paxos consensus有多轮，只要最终的学习值阶段没有达成所有的一致，就继续一轮一轮的做下去。

#####选择值（2阶段4步）：

**值（value）代表的是某种操作，而不是简单的一个字面意义上的变量**，而且可以是任意操作，例如“修改某个变量的值为某个值”、“设置当前 primary 为某个节点”等。Paxos协议中统一将这些操作抽象为 value。

选择值过程是proposer和大多数（majority）的acceptor之间的交互 

1a. proposer向majority acceptor 提出prepare request，并声明当前提议号n

1b. acceptor收到prepare后，检查是否response过更高的prepare，否则回复proposer：承诺不会接受提议号小于n的提议；同时返回已接受的最大的提议（含提议号和提议值）-如果有的话，没有就返回null；promise(n,an,av)

2a. proposer收到半数以上acceptor的promise后，发出accept request(n, v)，但其中的值有两种：

1）收到的promise全部都是null，即acceptor都没有接受过其他提议，则可自由任选值

2）收到的promise中有已接受的值，则后者认可前者，将值替换为接受的小于n的提议中最大的提议值

**PS：单纯只替换提议值是不够的，还需要更新该提议值被accepted时的提议号**

>如频繁宕机情况：
>
>

2b. 收到提议号为n的accept  request后，如果没有回复过其他更高的提议号就接受该值（存储），否则拒绝该请求

（有些类似于两阶段提交的概念？？）

最后一步的acceptor收到accept request后存储新值（表明被批准accepted了），不需要验证是大多数acceptor收到了，可能只有一个acceptor收到了；不过在之后的学习值过程中会要求大多数的acceptor给learner发送了值，从而反向验证了收到accept request并存储下来的acceptor是majority



1、选择值的过程中，可能导致“不一致”，但再经过几轮的提议后，最终达成一致，确定某个value；

2、前一个proposer的prepare request可能会被另一个proposer的prepare request直接取消，从而提出了一个更高提议号的prepare request，但又取消了另一个刚提出的prepare，这样不断的相互取消就产生了活锁（对于活锁，我的理解是它不像死锁，陷入相互等待资源，必须等新资源补充；活锁是只要其中一个proposer稍缓或者停下来，就可以恢复正常，因为活锁产生的原因在于：高轮数的提议可以抢占低轮数的提议，而这又恰恰是为了防止死锁的产生）

所以为避免活锁，可规定只有一个proposer可以提议，这个proposer即为distinguished proposer（但这样不是变成类似于只有一个提议者，然后提议之后再广播给所有的模式了吗？还需要共识吗？老师：共识的关键在于又多个acceptor，而不是又多个proposer）

3、选择值过程中，**后者认同前者**，防止出现后者提出的新值覆盖原有被批准的旧值，始终无法达成一致

##### 学习值

选择值之后，只有大多数（majority）的acceptor接受了值，学习值过程，即将选中的值告知所有的learner；

默认是这大多数的acceptor会给所有的learner发送值，所以消息总数为nums of majority acceptor * num of learner，网络占用较大，所以可以选择某一个learner为distinguished learner，这大多数的acceptor将值发给distinguished  learner，该learner收到大多数acceptor的值后会向剩余的所有learner发送该值，消息总数为nums of majority  acceptor + nums of learner；

实践中，选择某进程，该进程的proposer成为distinguished proposer，learner成为distinguished learner，这个被选中的进程即为**leader**





#### Multi Paxos（paxos algorithm / state machine）

等同于paxos consensus的多次执行，每次paxos consensus的执行即为一个实例，每个实例拥有一个实例编号

1、独立实例的paxos

2、共用第一次prepare的paxos



#### 复制状态机 replicated deterministic state machine





目前问题：paxos的应用 -- 选择值和学习值具体怎么用到分布式场景，多个acceptor达成共识然后呢？复制状态机让多个副本之间的命令顺序一致，paxos具体怎么做到？

Q2：

![PaxosQuestion1](F:\Markdown\研一上\图片\PaxosQuestion1.png)



kspace paxos代码先忽略存储、网络，关注paxos中存储了哪些部分，更新了哪些部分





#### Raft（复制日志算法）

与paxos比较的话，raft的term（任期）更像是一个跨多个实例、多次提议的选举分界线（或者leader分界线）

##### 投票策略（& election restriction）：

candidate先将自身term+1，然后发出RequestVote RPC，此RPC中会携带```term、日志中最后一条entry的term、日志中最后一条entry的index```；

follower首先判断自身currentTerm是否小于RPC中term，如果currentTerm > term，return false；

通过currentTerm判断后，检查RPC中最后一条entry的term和index与自身保存的最后一条entry的term及index的大小，RPC中term更大，则通过，两者term相同，则检查index，RPC中index更大，亦通过



##### 一致性检查

raft中认为日志具有特性：两个不同日志中的两个entry拥有相同的term和index，则这两个entry 前面的所有entry也全部相同；

为保证该特性，append entry时需要做一致性检查：raft中通过找到leader和follower两者最后达成一致的entry，然后删除follower在此entry之后的所有entry，用leader中的后续entry覆盖

###### 具体操作：

leader发送append RPC时会附带上当前entry的前一条entry的term和index，如果follower在自身entry中找不到RPC中附带的entry term和index，则返回append调用失败；leader继续把前一个entry发送给follower查找，直到append调用成功；如果append调用成功，则说明leader和follower已经找到最后达成一致的地点，leader从这个entry开始逐个调用append RPC，即用leader中的entry覆盖follower中原有的entry

故raft中不存在follower到leader的数据流动，leader不需要通过follower来保持数据的最新性



而为了标识leader为follower附加entry的位置，leader为每个follower维护一个nextIndex（直接一致性 检查时匹配的index+1，不行吗？？），当leader刚刚选出时，该值默认为leader的log最后一条entry的下一个位置，如果一致性检查未通过（即AppendEntriesRPC 返回false），则将nextIndex向前移动一位（修改相应前一条entry），再次发送AppendRPC，继续一致性检查，直到调用成功，将nextIndex对应entry覆盖至follower



##### not commiting entries from previous terms

新leader会保存旧leader时的entry，但不会提交这些已达多数的entry，当有新的entry到达时，并达到提交条件时，会自动提交起那么那些未提交的entry（根据一致性检查）

>新leader启动后如何判定后，旧的entry是否已提交？



##### 相关RPC

Figure 2的图看懂，搞清楚具体选举、复制策略；以及相应存储的数据

###### 存储状态

###### RequestVote RPC

###### AppendEntries RPC







Figure8：

一个已经在多数节点中存储（还未commit）的entry仍然可能在之后被覆盖



#### ZAB（原子广播算法）

首要进程：

leader：



zab中没有提议号，epoch和zid更像是任期和实例号





zab需要保证在某个副本上已被投递的提议，在其他副本上也会被提交；

1、为什么会出现这个问题？

leader中执行提交操作和follower中提交操作是异步的。

zab 的leader在收到多数的ack后，即认为该提议为已提交状态，会执行对应提交操作，同时发送commit请求；而follower是在收到leader的commit请求后，知道状态为已提交状态，会执行提交操作。所以如果leader再执行完提交操作后宕机，commit请求未来及发出，则此提议在leader所在副本已经被投递，但其他副本处于未被投递（还没执行提交操作）。所以根源来自于leader和follower的提交异步

2、raft中如何处理这个问题？

raft中leader提交entry（即应用此entry至状态机）后宕机，而之前响应leader的follower还未提交；在新的一轮中选举中，若之前从未响应过旧leader的follower首先发起投票请求，根据raft的投票策略（election restriction），此投票请求会被拒绝，从而新的leader只会在响应过旧leader的majority follower中选出。然后根据raft不直接提交旧leader的entry的要求，此entry会在下一次entry提交时同时被提交。

论文Figure8例子，raft似乎是通过拒绝新leader提交旧term的entry，当有新的entry追加进入，达到提交条件后，间接提交旧entry的方式，**也就是说允许已达到多数的entry（但未提交）的entry被删除！！**；（~~但是无法规避，如只有leader提交了该entry，而后期新的leader直接覆盖这个已提交的entry，更加错误了！！~~，后期的leader只会在已经有entry的那几个节点产生）。在paxos中达到多数的提议值是不会被删除的，因为如果新的proposer提出新值，acceptor返回的promise(n, (旧值的提议号，旧值))；accept消息中会更新（必须更新，否则可能出现已占多数的提议值被删除）这个旧值对应的提议号，即accept(n，(n，旧值))；有些类似于在raft中已占多数但未提交的旧entry在新的term中，该entry的term会被更新；当然实际的raft放弃了这种方案，选择了保持entry的term始终不变。

