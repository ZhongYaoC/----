1、multi paxos中我的理解为简单的多次提案`propose`公用同一个第一阶段P1，然后每个提案只需要不断推进第二阶段P2即可，只要proposer不出现故障，这个过程可以一直持续下去，这也让我认为multi paxos中leader是不可或缺的

但是看到很多资料或实现，均认为multi paxos中的这种方式并不依赖于leader或者说leader 并不是 multi-paxos 的必须项

为什么？？



2、Paxos Lease论文中提到的启动前等待Ms，以达成保证？keyspace实现中AcquireLeaseTimer的启动中似乎的确会等待一个时间段，但我一直没有搞清楚这样做的意义，如果所有的Lease发起方均等待Ms，那么不仍旧属于同时发起？如果本来两节点发起的顺序可能导致“活锁”，那么同时等待Ms不仍然可能导致“活锁”吗？



3、

之前思考一些paxos追赶、快照问题时，总是陷入一些思维误区：认为还是在基础的paxos上运行，认为实例号仅一个，但是为了提高效率，paxos可采用batch propose，也就是一次提案号涉及多个提案（或者说多个实例号，这就得看具体实现了），如果paxos运行中可以有两个实例号，一个是当前提案号中最初始\最末尾的实例，这个实例可以作为整个提案的概括，一个是正在learn/commit中的实例号，毕竟就算经过2. phase提交确定了这一批实例，但学习的时候不能保证节点正常运行期间，这一批实例全部learn/commit完毕（这其中涉及能否将这一批实例，如10个，一次性学习完毕），可能只有部分学习完毕，那么节点重启后，剩余的实例势必需要追赶（大多数的acceptor节点中已经持久化这一批实例，但未学习）

如果学习完毕的value才会对客户端响应结果，那似乎影响不大，新节点当选leader后，重新执行acceptor中剩余部分实例的learn阶段即可，如果新的提案中的实例号，追赶的实例号将会交织在一起，更加混乱，处理起来需要思考的点更多

但无论如何，acceptor中需要区分清楚已提交实例commited paxos id和持久化的最大实例，前者代表已经learn完毕的实例号，后者代表那一批次实例中最大实例号





4、pipeline问题

无论是phx还是阿里的z-paxos，为提高效率均提到pipeline流水线方式（z-paxos最后舍弃了pipeline，而是实现一个串行依次确定一个值），而使用场景均放置在learn阶段，到底什么叫pipeline









5、

冷启动时如何决定主从，之前实验中实现的不论是PaxosLease还是FastElection，冷启动时都是选择手动指定某一个节点发起选举，或者FastElection里干脆手动手动指定了一个节点作为默认leader（而且因此问题，引入了一个新的纠错报文，因为现在配置文件中的leader宕机后重新启动，会再次认为自身是leader），因为这两个算法中一直存在怎么保证各个节点同时发起选举，从而选出数据最新的节点的问题



采取类zab中的，先选出一个节点作为leader，后续发现此节点数据较陈旧，立即重新选举？？那么冷启动的时候，先选出一个节点指随机指定一个？？？

冷启动时，其实之前想到牛角尖去了，冷启动时所有节点都处于同一数据状态，那么随便一个都可以当选，不存在某个节点数据较为陈旧，因为启动较慢导致之后才加入的节点，同样发出looking leader报文即可，之后作为follower加入集群；不过这样的话，要不要选出leader之后sleep一段时间，尽可能让集群中所有节点都加入paxos，然后才进入下一步提案，否则可能节点间从一开始就需要追赶



20220520 因为paxos中acceptor存在持久化，那么冷启动时是带数据启动的，那么就需要考虑让数据最新的当选





6、

learn到一个实例及对应值后，并不需要落盘，那么这部分



7、

利用paxoslease选举时，新的leader申请方提出的提案，只有在acceptor的上一租期已经过期时，才能生效，这就导致时效性上略差；因为新提案的提出是由learner负责，而learner的过期时间是必然小于acceptor的过期时间，这就导致最早提出的提案，在进入2a之后被丢弃，直到半数以上acceptor的租期也过期





8、

paxos和paxos lease相比较之下，有一个区别在于定时器的设置，paxos 在第一阶段和第二阶段分别设置了定时器，处理两阶段的超时情况，而paxos lease 仅在第一阶段设置了一个总体定时器，用于处理选举失败的情况，所以当续租时，它需要从第一阶段重新开始，而不是从第二阶段直接开始，否则若遭遇从第二阶段发起的报文丢失等情况导致选举失败，那么就无法对选举失败做出有效处理，很可能导致选举永远停止，集群始终处于无主状态。

那么为什么对定时器做出如此改变呢？能否回到paxos的分阶段定时器，然后再附加一个两阶段的选举总定时器？

不过，如此操作下的确存在定时器重叠和冗余现象


